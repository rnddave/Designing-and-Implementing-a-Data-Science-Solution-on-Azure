{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if I see interesting code bits I will keep a copy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3\n",
    "\n",
    "# Load a library to do the hard work for us\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# First, we define our formula using a special syntax\n",
    "# This says that boot_size is explained by harness_size\n",
    "formula = \"boot_size ~ harness_size\"\n",
    "\n",
    "# Create the model, but don't train it yet\n",
    "model = smf.ols(formula = formula, data = dataset)\n",
    "\n",
    "# Note that we have created our model but it does not \n",
    "# have internal parameters set yet\n",
    "if not hasattr(model, 'params'):\n",
    "    print(\"Model selected but it does not have parameters set. We need to train it!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The cost (objective) function\n",
    "Our next step is to create a cost function (objective function).\n",
    "\n",
    "These functions in supervised learning compare the model's estimate to the correct answer. In our case, our label is temperature, so our cost function will compare the estimated temperature to temperatures seen in the historical records.\n",
    "\n",
    "https://learn.microsoft.com/en-gb/training/modules/introduction-to-classical-machine-learning/3-exercise-separate-data-test-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(actual_temperatures, estimated_temperatures):\n",
    "    '''\n",
    "    Calculates the difference between actual and estimated temperatures\n",
    "    Returns the difference, and also returns the squared difference (the cost)\n",
    "\n",
    "    actual_temperatures: One or more temperatures recorded in the past\n",
    "    estimated_temperatures: Corresponding temperature(s) estimated by the model\n",
    "    '''\n",
    "\n",
    "    # Calculate the difference between actual temperatures and those\n",
    "    # estimated by the model\n",
    "    difference = estimated_temperatures - actual_temperatures\n",
    "\n",
    "    # Convert to a single number that tells us how well the model did\n",
    "    # (smaller numbers are better)\n",
    "    cost = sum(difference ** 2)\n",
    "\n",
    "    return difference, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop\n",
    "Let's put these components together so that they train the model.\n",
    "\n",
    "First, let's make a function that performs one iteration of training. Read each step carefully in the following code. If you want, add some print() statements inside the method to help you see the training in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iteration(model_inputs, true_temperatures, last_cost:float):\n",
    "    '''\n",
    "    Runs a single iteration of training.\n",
    "\n",
    "\n",
    "    model_inputs: One or more dates to provide the model (dates)\n",
    "    true_temperatues: Corresponding temperatures known to occur on those dates\n",
    "\n",
    "    Returns:\n",
    "        A Boolean, as to whether training should continue\n",
    "        The cost calculated (small numbers are better)\n",
    "    '''\n",
    "\n",
    "    # === USE THE MODEL ===\n",
    "    # Estimate temperatures for all data that we have\n",
    "    estimated_temperatures = model.predict(model_inputs)\n",
    "\n",
    "    # === OBJECTIVE FUNCTION ===\n",
    "    # Calculate how well the model is working\n",
    "    # Smaller numbers are better \n",
    "    difference, cost = cost_function(true_temperatures, estimated_temperatures)\n",
    "\n",
    "    # Decide whether to keep training\n",
    "    # We'll stop if the training is no longer improving the model effectively\n",
    "    if cost >= last_cost:\n",
    "        # Stop training\n",
    "        return False, cost\n",
    "    else:\n",
    "        # === OPTIMIZER ===\n",
    "        # Calculate updates to parameters\n",
    "        intercept_update, slope_update = optimizer.get_parameter_updates(model_inputs, cost, difference)\n",
    "\n",
    "        # Change the model parameters\n",
    "        model.slope += slope_update\n",
    "        model.intercept += intercept_update\n",
    "\n",
    "        return True, cost\n",
    "\n",
    "print(\"Training method ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# box plots\n",
    "\n",
    "he box plot shows the distribution of the grade values in a different format to the histogram. The *box* part of the plot shows where the inner two *quartiles* of the data reside - so in this case, half of the grades are between approximately 36 and 63. The *whiskers* extending from the box show the outer two quartiles; so the other half of the grades in this case are between 0 and 36 or 63 and 100. The line in the box indicates the *median* value.\n",
    "\n",
    "For learning, it can be useful to combine histograms and box plots, with the box plot's orientation changed to align it with the histogram (in some ways, it can be helpful to think of the histogram as a \"front elevation\" view of the distribution, and the box plot as a \"plan\" view of the distribution from above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that we can re-use\n",
    "def show_distribution(var_data):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    # Get statistics\n",
    "    min_val = var_data.min()\n",
    "    max_val = var_data.max()\n",
    "    mean_val = var_data.mean()\n",
    "    med_val = var_data.median()\n",
    "    mod_val = var_data.mode()[0]\n",
    "\n",
    "    print('Minimum:{:.2f}\\nMean:{:.2f}\\nMedian:{:.2f}\\nMode:{:.2f}\\nMaximum:{:.2f}\\n'.format(min_val,\n",
    "                                                                                            mean_val,\n",
    "                                                                                            med_val,\n",
    "                                                                                            mod_val,\n",
    "                                                                                            max_val))\n",
    "\n",
    "    # Create a figure for 2 subplots (2 rows, 1 column)\n",
    "    fig, ax = plt.subplots(2, 1, figsize = (10,4))\n",
    "\n",
    "    # Plot the histogram   \n",
    "    ax[0].hist(var_data)\n",
    "    ax[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Add lines for the mean, median, and mode\n",
    "    ax[0].axvline(x=min_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=mean_val, color = 'cyan', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=med_val, color = 'red', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=mod_val, color = 'yellow', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=max_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
    "\n",
    "    # Plot the boxplot   \n",
    "    ax[1].boxplot(var_data, vert=False)\n",
    "    ax[1].set_xlabel('Value')\n",
    "\n",
    "    # Add a title to the Figure\n",
    "    fig.suptitle('Data Distribution')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "# Get the variable to examine\n",
    "col = df_students['Grade']\n",
    "# Call the function\n",
    "show_distribution(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression\n",
    "\n",
    "Fortunately, you don't need to code the regression calculation yourself - the **SciPy** package includes a **stats** class that provides a **linregress** method to do the hard work for you. This returns (among other things) the coefficients you need for the slope equation - slope (*m*) and intercept (*b*) based on a given pair of variable samples you want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#\n",
    "df_regression = df_sample[['Grade', 'StudyHours']].copy()\n",
    "\n",
    "# Get the regression slope and intercept\n",
    "m, b, r, p, se = stats.linregress(df_regression['StudyHours'], df_regression['Grade'])\n",
    "print('slope: {:.4f}\\ny-intercept: {:.4f}'.format(m,b))\n",
    "print('so...\\n f(x) = {:.4f}x + {:.4f}'.format(m,b))\n",
    "\n",
    "# Use the function (mx + b) to calculate f(x) for each x (StudyHours) value\n",
    "df_regression['fx'] = (m * df_regression['StudyHours']) + b\n",
    "\n",
    "# Calculate the error between f(x) and the actual y (Grade) value\n",
    "df_regression['error'] = df_regression['fx'] - df_regression['Grade']\n",
    "\n",
    "# Create a scatter plot of Grade vs StudyHours\n",
    "df_regression.plot.scatter(x='StudyHours', y='Grade')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(df_regression['StudyHours'],df_regression['fx'], color='cyan')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
